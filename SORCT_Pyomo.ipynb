{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<!--NOTEBOOK_HEADER-->\n",
        "*This notebook contains material from [Sparsity in optimal randomized classification trees](https://idus.us.es/handle/11441/107840)."
      ],
      "metadata": {
        "id": "WdCG011jbB_6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "ysXHrK9dbFRh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "REWafWBGCe-F",
        "nbpages": {
          "level": 2,
          "link": "[7.1.1 Imports](https://jckantor.github.io/ND-Pyomo-Cookbook/07.01-Parameter-Estimation-Catalytic-Reactor.html#7.1.1-Imports)",
          "section": "7.1.1 Imports"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1899f12-cff0-4c5b-cc79-296bb85def9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 9.7 MB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 49 kB 2.0 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import scipy.stats as stats\n",
        "import scipy.optimize as optimize\n",
        "\n",
        "import shutil\n",
        "import sys\n",
        "import os.path\n",
        "\n",
        "if not shutil.which(\"pyomo\"):\n",
        "    !pip install -q pyomo\n",
        "    assert(shutil.which(\"pyomo\"))\n",
        "\n",
        "if not (shutil.which(\"ipopt\") or os.path.isfile(\"ipopt\")):\n",
        "    if \"google.colab\" in sys.modules:\n",
        "        !wget -N -q \"https://ampl.com/dl/open/ipopt/ipopt-linux64.zip\"\n",
        "        !unzip -o -q ipopt-linux64\n",
        "    else:\n",
        "        try:\n",
        "            !conda install -c conda-forge ipopt \n",
        "        except:\n",
        "            pass\n",
        "\n",
        "assert(shutil.which(\"ipopt\") or os.path.isfile(\"ipopt\"))\n",
        "\n",
        "from pyomo.environ import *\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Description"
      ],
      "metadata": {
        "id": "7uVOdp5AsEk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook uses [Pyomo](http://www.pyomo.org/) to implement the sparsity in optimal randomized classification trees model (SORCT) and tests it using [The Iris Dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) according to the the paper [Sparsity in optimal randomized classification trees](https://idus.us.es/handle/11441/107840). The SORCT based on oblique cuts proposes a continuous optimization approach for both local and global sparsity, where all decision branches are optimised simultaneously, and has demonstrated a more thorough performance when compared to traditional decision trees like CARTs using the greedy algorithm, which focuses on local sparsity."
      ],
      "metadata": {
        "id": "LOe2AFXDwRZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree"
      ],
      "metadata": {
        "id": "CG3PQjukVik1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From 2.2.1 of the [paper](https://idus.us.es/bitstream/handle/11441/107840/Sparsity%20in%20optimal%20randomized%20classification%20trees.pdf?sequence=1&isAllowed=y):\n",
        "\n",
        "| *Parameters* <img width=50/> | <img width=250/> |\n",
        "| :--------------------------- | :- |\n",
        "| $D$ | depth of the binary tree |\n",
        "| $N$ | number of individuals in the training sample |\n",
        "| $p$ | number of predictor variables |\n",
        "| $K$ | number of classes |\n",
        "| $\\{(xi, yi)\\}_{1\\leq i\\leq N}$ | training sample, where $x_i ∈ [0, 1]^p$ and <br> $y_i \\in \\{1, . . ., K\\}$ |\n",
        "| $I_k$ | set of individuals in the training sample <br> belonging to class $k, k = 1, . . ., K$ |\n",
        "| $W_{y_{i^k}}$ | misclassification cost incurred when <br> classifying an individual $i$, whose class is $y_i$ , <br> in class $k, yi, i = 1, . . ., N, k = 1, . . ., K$ |\n",
        "| $F (·)$ | univariate continuous CDF centered at 0, used <br> to define the probabilities for an individual to <br> go to the left or the right child node in the <br> tree. We will assume that $F$ is the CDF of a <br> continuous random variable with density $f$ |\n",
        "| $\\lambda ^L \\ge 0$ | local sparsity regularization parameter |\n",
        "| $\\lambda ^G \\ge 0$ | global sparsity regularization parameter |\n",
        "\n",
        "<br>\n",
        "\n",
        "| *Nodes* <img width=77/> | <img width=250/> |\n",
        "| :---------------------- | :- |\n",
        "| $\\tau _ B$ | set of branch nodes |\n",
        "| $\\tau _ L$ | set of leaf nodes |\n",
        "| $N_L(t)$ | set of ancestor nodes of leaf node $t$ whose <br> left branch takes part in the path from the <br> root node to leaf node $t$, $t \\in \\tau$ |\n",
        "| $N_L(t)$ | set of ancestor nodes of leaf node $t$ whose <br> right branch takes part in the path from the <br> root node to leaf node $t$, $t \\in \\tau _L$ |\n",
        "\n",
        "<br>\n",
        "\n",
        "| *Decision variables* <img width=15/> | <img width=250/> |\n",
        "| :----------------------------------- | :- |\n",
        "| $a_{jt} \\in [−1, 1]$ | coefficient of predictor variable $j$ in the <br> oblique cut at branch node $t \\in \\tau _B$, with **$a$** <br> being the $p × \\mid \\tau _B \\mid$ matrix of these coefficients <br> , $a = (a_{jt}) _{j=1,...,p, t∈τB}$. The expressions $a_j·$ <br> and **$a_t$** will denote the $j$-th row and the $t$-th <br> column of **$a$**, respectively |\n",
        "| $\\mu_t \\in [−1, 1]$ | location parameter at branch node $t \\in \\tau_B$, $\\mu$ <br> being the vector that comprises every $\\mu_t$, i.e., <br> $\\mu = (\\mu_t)_{t\\in \\tau_B}$ |\n",
        "| $C_{kt}$ | probability of being assigned to class label <br> $k \\in {1, . . ., K}$ for an individual at leaf node <br> $t$, $t \\in \\tau_L$, being the $K × \\mid \\tau_L\\mid$ matrix such that <br> $C = (C_{kt})_{k=1,...,K, t\\in \\tau_L}$ |\n",
        "\n",
        "<br>\n",
        "\n",
        "| *Probabilities* <img width=45/> | <img width=250/> |\n",
        "| :------------------------------ | :- |\n",
        "| $p_{it}($**$a$**$_{·t},\\mu_t)$ | probability of individual $i$ going down the left <br> branch at branch node $t$. Its expression is <br> $p_{it}($**$a$**$_{·t},\\mu_t) = F(\\frac{1}{p}$**$a$**$^T_{·t}x_i-$**$\\mu$**$_t)$, <br> $i = 1,...,N, t \\in \\tau_B$ |\n",
        "| $P_{it}($**$a$**$,\\bf{\\mu})$ | probability of individual $i$ failing into leaf <br> node $t$. Its expression is <br> $P_{it}($**$a$**$,\\mu) = \\prod\\limits_{t_l \\in N_L(t)} p_{it_l}($**$a$**$_{·t_l},\\mu_{t_l}) $<br>$\\prod\\limits_{t_l \\in N_R(t)} (1 - p_{it_r}($**$a$**$_{·t_r},\\mu_{t_r})) $, <br> $i = 1,...,N, t \\in \\tau_L$ |\n",
        "| $g(a,\\mu,C)$ | expected misclassification cost over the <br> training sample. Its expression is <br> $g(a,\\mu,C) = \\frac{1}{N}\\sum\\limits^N_{i=1}\\sum\\limits_{t \\in \\tau_L} P_{it}(a, \\mu) \\sum\\limits^K_{k=1} W_{y_{i^k}}C_{kt}$ |\n",
        "\n",
        "<br>\n",
        "\n",
        "| *Objective* <img width=262/> | <img width=50/> |\n",
        "| :--------------------------- | :- |\n",
        "| min $g(a,\\mu,C) + \\lambda^L\\sum\\limits^p_{j=1}\\|a_{j·}\\|_1 + \\lambda^G\\sum\\limits^p_{j=1}\\|a_{j·}\\|_\\infty $ | |\n",
        "\n",
        "<br>\n",
        "\n",
        "| *Constraint* <img width=257/> | <img width=50/> |\n",
        "| :---------------------------- | :- |\n",
        "| $\\sum\\limits^K_{k=1} C_{kt} = 1, t \\in \\tau_L$ | |\n",
        "| $\\sum\\limits_{t \\in \\tau_L} C_{kt} \\ge 1, k=1,...,K$ | |\n",
        "| $a_{jt} \\in [-1,1], j=1,...,p, t\\in \\tau_B$ | |\n",
        "| $\\mu_t \\in [-1,1], t\\in \\tau_B$ | |\n",
        "| $C_{kt} \\in [0,1], k=1,...,K, t\\in \\tau_L$ | |\n",
        "\n",
        "<br>\n",
        "\n",
        "Achieved using Pyomo below:\n"
      ],
      "metadata": {
        "id": "XboL1U5NddvL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# optimal randomized classification trees model\n",
        "\n",
        "def orct(X, y, depth, F_turn = 128, l_reg = 0.1, g_reg = 1):\n",
        "    n_instances, n_features = X.shape\n",
        "    \n",
        "    m = ConcreteModel()\n",
        "\n",
        "    # D depth\n",
        "    m.D = Set(initialize=list(range(depth)))\n",
        "    # p predictor variables #s\n",
        "    m.p = Set(initialize=list(range(n_features)))\n",
        "    # N training sample #s\n",
        "    m.N = Set(initialize=list(range(n_instances)))\n",
        "    # k categories\n",
        "    m.k = Set(initialize=list(range(len(np.unique(y)))))\n",
        "    # training samples\n",
        "    train_set = {(n, p): X[n,p] for n in m.N for p in m.p}\n",
        "    m.X = Param(m.N, m.p, within=Reals, initialize=train_set)\n",
        "    m.y = Param(m.N, within=Reals, initialize={n: y[n] for n in m.N}) # category k \n",
        "    m.training_samples = Param(m.N, initialize={n: (X[n], y[n]) for n in m.N})\n",
        "    # I_k\n",
        "    idic = {}\n",
        "    for j in m.training_samples:\n",
        "        if m.training_samples[j][1] not in idic:\n",
        "            idic[m.training_samples[j][1]] = [tuple(m.training_samples[j][0])]\n",
        "        else:\n",
        "            idic[m.training_samples[j][1]].append(tuple(m.training_samples[j][0]))\n",
        "    m.Ik = Set(m.k,initialize=idic)\n",
        "    # W_yik\n",
        "    m.W_yik = Param(m.k, m.N, within=Binary, initialize={(k, n): int(m.y[n]!=k) for k in m.k for n in m.N})\n",
        "\n",
        "\n",
        "    # F\n",
        "    def F(x):\n",
        "        return 1./(1. + exp(-x*F_turn))\n",
        "\n",
        "\n",
        "    # Nodes\n",
        "    m.tb = Set(initialize=list(range(1,2**depth)))\n",
        "    m.tl = Set(initialize=list(range(2**depth,2*2**depth)))\n",
        "    def Nl_Nr(node):\n",
        "        Nl_list = []\n",
        "        Nr_list = []\n",
        "        while node>1:\n",
        "            if node%2 == 0:\n",
        "                node = node//2\n",
        "                Nl_list.append(node)\n",
        "            else:\n",
        "                node = (node-1)//2\n",
        "                Nr_list.append(node)\n",
        "        return (Nl_list,Nr_list)\n",
        "    m.Nl = Set(m.tl,initialize={n: Nl_Nr(n)[0] for n in m.tl})\n",
        "    m.Nr = Set(m.tl,initialize={n: Nl_Nr(n)[1] for n in m.tl})\n",
        "\n",
        "\n",
        "    # VARIABLES\n",
        "    m.a_jt = Var(m.p, m.tb, bounds=[-1,1], within=Reals)\n",
        "    m.a_jt_Max = Var(m.p, within=Reals) # max() for each vactor a_j.\n",
        "    m.mu_t = Var(m.tb, bounds=[-1,1], within=Reals)\n",
        "    m.C_kt = Var(m.k, m.tl, bounds=[0,1], within=Reals)\n",
        "\n",
        "\n",
        "    # Probabilities\n",
        "    def p_it(model, tb, i):\n",
        "        return F((1/n_features) * (sum(model.a_jt[p,tb]*model.X[i, p] for p in model.p)) - model.mu_t[tb])\n",
        "    m.p_it = Expression(m.tb, m.N, rule=p_it)\n",
        "\n",
        "    def P_it(model, tl, i):\n",
        "        return prod(p_it(model, nl, i) for nl in model.Nl[tl])*prod(1-p_it(model, nr, i) for nr in model.Nr[tl])\n",
        "    m.P_it = Expression(m.tl, m.N, rule=P_it)\n",
        "\n",
        "    def g(model, i, tl):\n",
        "        return model.P_it[tl,i] * sum(model.W_yik[k,i]*model.C_kt[k,tl] for k in model.k)\n",
        "    m.g = Expression(m.N, m.tl, rule=g)\n",
        "\n",
        "\n",
        "    # Objective\n",
        "    def cost_rule(model):\n",
        "        cost = 0.0\n",
        "        for i in model.N:\n",
        "          for tl in model.tl:\n",
        "            cost += model.g[i,tl]\n",
        "        # return cost\n",
        "        lreg = 0.0\n",
        "        greg = 0.0\n",
        "        for p in model.p:\n",
        "          greg += model.a_jt_Max[p]\n",
        "          for tb in model.tb:\n",
        "            lreg += model.a_jt[p,tb]\n",
        "        return cost+l_reg*lreg+g_reg*greg\n",
        "    m.cost = Objective(rule=cost_rule, sense=minimize)\n",
        "\n",
        "\n",
        "    # Constraint\n",
        "    def aRule(model,tl):\n",
        "        return sum(model.C_kt[k,tl] for k in model.k) == 1\n",
        "    m.Boundk_C_kt = Constraint(m.tl, rule=aRule)\n",
        "\n",
        "    def bRule(model,k):\n",
        "        return sum(model.C_kt[k,tl] for tl in model.tl) >= 1\n",
        "    m.Boundt_C_kt = Constraint(m.k, rule=bRule)\n",
        "\n",
        "    def a_jt_Max_Rule(model, p, tb): # for global sparsity regularization parameter\n",
        "        return model.a_jt_Max[p] >= model.a_jt[p, tb]\n",
        "    m.Bound_a_jt_Max = Constraint(m.p, m.tb,rule=a_jt_Max_Rule)\n",
        "\n",
        "\n",
        "    return m\n",
        "\n",
        "\n",
        "# optimal randomized classification trees prediction function\n",
        "\n",
        "def orct_predict(model, X, F_turn=128):\n",
        "  n_instances, n_features = X.shape\n",
        "  def F(x):\n",
        "        return 1./(1. + exp(-x*F_turn))\n",
        "\n",
        "  P_it = np.zeros((len(model.tb)+len(model.tl), n_instances))\n",
        "  for tb in model.tb:\n",
        "    for i in range(n_instances):\n",
        "      P_it[tb-1, i] = F((1/n_features) * (sum(model.a_jt[p,tb]()*X[i, p] for p in model.p)) - model.mu_t[tb]())\n",
        "  for tl in model.tl:\n",
        "    for i in range(n_instances):\n",
        "      P_it[tl-1, i] = prod(P_it[nl-1, i] for nl in model.Nl[tl])*prod(1-P_it[nr-1, i] for nr in model.Nr[tl])\n",
        "  return [np.argmax([sum(P_it[tl-1, n]*model.C_kt[cl, tl]() for tl in model.tl) for cl in model.k]) for n in range(n_instances)]"
      ],
      "metadata": {
        "id": "sMfyi3O1ZgvB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Iris dataset."
      ],
      "metadata": {
        "id": "BJgLiCfEKu-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iris dataset\n",
        "X, y = datasets.load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
      ],
      "metadata": {
        "id": "j2RywFx_cx9Z"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solving by ipopt."
      ],
      "metadata": {
        "id": "hZKBYM5UxRTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "logging.getLogger('pyomo.core').setLevel(logging.ERROR)\n",
        "\n",
        "orct_model = orct(X_train, y_train, depth=3)\n",
        "solver = SolverFactory('ipopt')\n",
        "results = solver.solve(orct_model, tee=True)\n",
        "print(results)"
      ],
      "metadata": {
        "id": "J6fSux03ZuGx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c19e4e4d-b7a5-4276-db14-0563e7b609b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ipopt 3.12.13: \n",
            "\n",
            "******************************************************************************\n",
            "This program contains Ipopt, a library for large-scale nonlinear optimization.\n",
            " Ipopt is released as open source code under the Eclipse Public License (EPL).\n",
            "         For more information visit http://projects.coin-or.org/Ipopt\n",
            "******************************************************************************\n",
            "\n",
            "This is Ipopt version 3.12.13, running with linear solver mumps.\n",
            "NOTE: Other linear solvers might be more efficient (see Ipopt documentation).\n",
            "\n",
            "Number of nonzeros in equality constraint Jacobian...:       24\n",
            "Number of nonzeros in inequality constraint Jacobian.:       80\n",
            "Number of nonzeros in Lagrangian Hessian.............:      715\n",
            "\n",
            "Total number of variables............................:       63\n",
            "                     variables with only lower bounds:        0\n",
            "                variables with lower and upper bounds:       59\n",
            "                     variables with only upper bounds:        0\n",
            "Total number of equality constraints.................:        8\n",
            "Total number of inequality constraints...............:       31\n",
            "        inequality constraints with only lower bounds:        3\n",
            "   inequality constraints with lower and upper bounds:        0\n",
            "        inequality constraints with only upper bounds:       28\n",
            "\n",
            "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
            "   0  2.2399978e+00 9.70e-01 7.89e-01  -1.0 0.00e+00    -  0.00e+00 0.00e+00   0\n",
            "   1  7.4694329e+01 0.00e+00 1.67e+04  -1.7 1.66e+00   4.0 6.01e-03 1.00e+00h  1\n",
            "   2  7.4691800e+01 1.11e-16 2.25e+00  -1.7 6.84e-04   3.5 1.00e+00 1.00e+00f  1\n",
            "   3  7.4610511e+01 0.00e+00 6.18e+00  -1.7 5.98e-03   3.0 1.00e+00 1.00e+00f  1\n",
            "   4  7.4566087e+01 2.22e-16 7.01e+00  -1.7 2.35e-03   3.5 1.00e+00 1.00e+00f  1\n",
            "   5  7.4230841e+01 0.00e+00 1.06e+01  -1.7 1.11e-02   3.0 1.00e+00 1.00e+00f  1\n",
            "   6  7.4042124e+01 0.00e+00 1.25e+01  -1.7 4.74e-03   3.4 1.00e+00 1.00e+00f  1\n",
            "   7  7.2958734e+01 0.00e+00 1.01e+02  -1.7 2.27e-02   2.9 1.00e+00 1.00e+00f  1\n",
            "   8  7.1779173e+01 1.11e-16 2.17e+01  -1.7 1.04e-02   3.4 1.00e+00 1.00e+00f  1\n",
            "   9  6.9457569e+01 2.22e-16 2.33e+01  -1.7 3.01e-02   2.9 1.00e+00 1.00e+00f  1\n",
            "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
            "  10  6.1765571e+01 1.11e-16 2.22e+01  -1.7 1.01e-01   2.4 1.00e+00 1.00e+00f  1\n",
            "  11  5.0718256e+01 0.00e+00 2.32e+01  -1.7 2.94e-01   1.9 1.00e+00 4.97e-01f  1\n",
            "  12  5.0236609e+01 0.00e+00 2.30e+01  -1.7 7.57e-01   1.5 1.00e+00 1.48e-02f  1\n",
            "  13  3.7295276e+01 2.22e-16 2.18e+01  -1.7 2.10e+00   1.0 8.63e-01 1.62e-01f  1\n",
            "  14  3.6958058e+01 2.22e-16 1.36e+01  -1.7 1.17e-01   1.4 1.00e+00 1.04e-01f  1\n",
            "  15  3.2228355e+01 1.11e-16 4.40e+01  -1.7 3.95e-01   0.9 1.00e+00 4.88e-01f  2\n",
            "  16  3.1522709e+01 2.22e-16 1.39e+01  -1.7 9.56e-03   3.2 1.00e+00 1.00e+00f  1\n",
            "  17  2.9176041e+01 0.00e+00 2.58e+02  -1.7 4.26e-02   2.7 1.00e+00 1.00e+00f  1\n",
            "  18  2.0661388e+01 1.11e-16 4.84e+01  -1.7 1.21e-01   2.2 1.00e+00 9.29e-01f  1\n",
            "  19  1.6350032e+01 1.11e-16 4.62e+01  -1.7 4.05e-01   1.7 7.70e-01 1.63e-01f  1\n",
            "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
            "  20  5.6420637e+00 0.00e+00 6.50e+01  -1.7 1.10e+00   1.3 9.20e-01 2.89e-01f  1\n",
            "  21  4.2620216e+00 2.22e-16 6.23e+01  -1.7 1.01e+00   0.8 9.94e-01 4.09e-02f  1\n",
            "  22  2.9570068e+00 2.22e-16 1.80e+01  -1.7 1.13e-01   0.3 1.00e+00 1.00e+00f  1\n",
            "  23  2.3172135e+00 2.22e-16 2.22e+01  -1.7 9.76e-01    -  3.75e-01 2.00e-01f  1\n",
            "  24  1.4137566e+00 1.11e-16 2.80e+01  -1.7 2.30e-01  -0.2 5.07e-01 1.00e+00f  1\n",
            "  25  1.2279654e+00 1.11e-16 9.97e+00  -1.7 5.61e-01   0.3 2.68e-01 1.44e-01f  2\n",
            "  26  1.0412836e+00 2.22e-16 9.57e+00  -1.7 1.46e+00    -  4.02e-01 4.33e-02f  1\n",
            "  27  2.0606150e-01 1.11e-16 7.78e+00  -1.7 3.27e+00    -  4.32e-01 2.84e-01f  1\n",
            "  28 -8.6741100e-03 0.00e+00 1.52e+00  -1.7 1.37e-01   0.7 1.00e+00 1.00e+00f  1\n",
            "  29 -1.0742061e+00 0.00e+00 1.14e+00  -1.7 1.19e+00    -  3.77e-01 4.61e-01f  1\n",
            "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
            "  30 -1.3531927e+00 0.00e+00 8.18e-01  -1.7 2.40e+00    -  5.44e-01 2.96e-01f  1\n",
            "  31 -1.4631406e+00 0.00e+00 2.60e-01  -1.7 4.19e-01    -  1.00e+00 5.16e-01f  1\n",
            "  32 -1.4749467e+00 0.00e+00 8.96e-01  -1.7 2.41e-01    -  1.00e+00 1.00e+00f  1\n",
            "  33 -1.6168823e+00 0.00e+00 2.54e-01  -1.7 4.32e-01    -  5.10e-01 1.00e+00f  1\n",
            "  34 -1.7081159e+00 2.22e-16 1.85e-01  -1.7 3.40e+00    -  2.83e-01 2.97e-01f  1\n",
            "  35 -2.3045845e+00 2.22e-16 1.38e-01  -2.5 1.90e-01    -  1.00e+00 8.26e-01f  1\n",
            "  36 -2.4707700e+00 2.22e-16 2.20e-01  -2.5 9.81e-01    -  1.00e+00 1.00e+00f  1\n",
            "  37 -2.4705718e+00 2.22e-16 4.67e-03  -2.5 3.17e-01    -  1.00e+00 1.00e+00f  1\n",
            "  38 -2.6034260e+00 1.11e-16 3.08e-02  -3.8 1.34e-01    -  8.92e-01 1.00e+00f  1\n",
            "  39 -2.6054285e+00 0.00e+00 5.15e-04  -3.8 2.64e-03    -  1.00e+00 1.00e+00f  1\n",
            "iter    objective    inf_pr   inf_du lg(mu)  ||d||  lg(rg) alpha_du alpha_pr  ls\n",
            "  40 -2.6129947e+00 1.11e-16 1.19e-03  -5.7 7.84e-03    -  1.00e+00 1.00e+00f  1\n",
            "  41 -2.6130136e+00 2.22e-16 6.60e-06  -5.7 8.33e-04    -  1.00e+00 1.00e+00f  1\n",
            "  42 -2.6131039e+00 2.22e-16 3.06e-02  -8.6 1.64e-04    -  1.00e+00 9.55e-01f  1\n",
            "In iteration 42, 1 Slack too small, adjusting variable bound\n",
            "  43 -2.6131056e+00 2.22e-16 1.33e+01  -8.6 8.09e-06    -  9.82e-01 4.60e-01f  1\n",
            "  44 -2.6131075e+00 2.22e-16 4.08e-10  -8.6 4.37e-06    -  1.00e+00 1.00e+00f  1\n",
            "\n",
            "Number of Iterations....: 44\n",
            "\n",
            "                                   (scaled)                 (unscaled)\n",
            "Objective...............:  -2.6131074849200000e+00   -2.6131074849200000e+00\n",
            "Dual infeasibility......:   4.0800735624876068e-10    4.0800735624876068e-10\n",
            "Constraint violation....:   2.2204460492503131e-16    2.2204460492503131e-16\n",
            "Complementarity.........:   4.7409252526233225e-09    4.7409252526233225e-09\n",
            "Overall NLP error.......:   4.7409252526233225e-09    4.7409252526233225e-09\n",
            "\n",
            "\n",
            "Number of objective function evaluations             = 47\n",
            "Number of objective gradient evaluations             = 45\n",
            "Number of equality constraint evaluations            = 47\n",
            "Number of inequality constraint evaluations          = 47\n",
            "Number of equality constraint Jacobian evaluations   = 45\n",
            "Number of inequality constraint Jacobian evaluations = 45\n",
            "Number of Lagrangian Hessian evaluations             = 44\n",
            "Total CPU secs in IPOPT (w/o function evaluations)   =      0.104\n",
            "Total CPU secs in NLP function evaluations           =      0.222\n",
            "\n",
            "EXIT: Optimal Solution Found.\n",
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Problem: \n",
            "- Lower bound: -inf\n",
            "  Upper bound: inf\n",
            "  Number of objectives: 1\n",
            "  Number of constraints: 39\n",
            "  Number of variables: 63\n",
            "  Sense: unknown\n",
            "Solver: \n",
            "- Status: ok\n",
            "  Message: Ipopt 3.12.13\\x3a Optimal Solution Found\n",
            "  Termination condition: optimal\n",
            "  Id: 0\n",
            "  Error rc: 0\n",
            "  Time: 0.4077877998352051\n",
            "Solution: \n",
            "- number of solutions: 0\n",
            "  number of solutions displayed: 0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Train error: \", accuracy_score(y_train, orct_predict(orct_model, X_train)))\n",
        "print(\"Test error: \",accuracy_score(y_test, orct_predict(orct_model, X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y2YoBbluJHOI",
        "outputId": "a896cf1d-bca3-43f0-fb09-82ce062060e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train error:  0.9910714285714286\n",
            "Test error:  0.9473684210526315\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "SORCT_Pyomo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}